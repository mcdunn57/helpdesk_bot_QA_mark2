{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe85fe38-7e19-4b9d-8c0d-221ad33d3bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mike/Documents/python_stuff/GeminiPro_API_tinkering/helpdesk_bot_QA\n"
     ]
    }
   ],
   "source": [
    " #print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ade3a1-6e12-4a45-aab9-823d245838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Installation ---\n",
    "# Run this cell first to install the required libraries.\n",
    "# !pip install langchain langgraph langchain_google_genai pandas faiss-cpu google-generativeai python-dotenv\n",
    "\n",
    "# --- 2. Imports and Setup ---\n",
    "import os\n",
    "import operator\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d304844f-60ef-45fb-87c3-99f72d52977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded and configured from .env file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. API Key and LLM Initialization ---\n",
    "# Ensure you have a .env file in the same directory with the line:\n",
    "# GOOGLE_API_KEY=\"your-actual-api-key\"\n",
    "load_dotenv(dotenv_path='/Users/mike/Documents/python_stuff/GeminiPro_API_tinkering/helpdesk_bot_QA/.env')\n",
    "\n",
    "try:\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in the .env file or environment variables. Please create a .env file and add your key (e.g., GOOGLE_API_KEY='your-key-here').\")\n",
    "    \n",
    "    # The API key is already set as an environment variable by load_dotenv(),\n",
    "    # but we also need to configure the native genai library.\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úÖ API key loaded and configured from .env file.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    # Stop execution if the key is not found\n",
    "    raise e\n",
    "\n",
    "# Initialize the LLM for LangChain (used by retriever) and a native Google model for direct calls\n",
    "# This uses the langchain wrapper, which we will bypass in the problematic step\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# This is the native Google library model, which we will use for direct, robust calls.\n",
    "native_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e368ee-8e43-44f4-bc25-a561403fb938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging chatbot responses to: user_output_log_20250826_164444.txt\n",
      "\n",
      "Refined RAG Chatbot is ready! Type your message or 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I need to reset my password.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü§î Executing node: intent_and_retrieve ---\n",
      "    Raw response from LLM: ```json\n",
      "{\n",
      "  \"rephrased_question\": \"How do I reset my password?\",\n",
      "  \"category\": \"account_management\"\n",
      "}\n",
      "```\n",
      "\n",
      "    Rephrased question for retrieval: 'How do I reset my password?'\n",
      "    Top document retrieved with relevance score: 0.6597\n",
      "    ‚úÖ Match is relevant enough.\n",
      "--- ‚úçÔ∏è Executing node: generate_refined_response ---\n",
      "Bot:\n",
      "To reset your password, go to the login page and click 'Forgot Password'. Follow the instructions sent to your email.\n",
      "\n",
      "--- \n",
      "**Summary of Agent's Actions:**\n",
      "* **Your Question:** \"I need to reset my password.\"\n",
      "* **Interpreted as:** \"How do I reset my password?\"\n",
      "* **Retrieved Document:** \"How do I reset my password?\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  How do I reset my password?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü§î Executing node: intent_and_retrieve ---\n",
      "    Raw response from LLM: ```json\n",
      "{\n",
      "  \"rephrased_question\": \"How do I reset my password?\",\n",
      "  \"category\": \"account_management\"\n",
      "}\n",
      "```\n",
      "\n",
      "    Rephrased question for retrieval: 'How do I reset my password?'\n",
      "    Top document retrieved with relevance score: 0.6597\n",
      "    ‚úÖ Match is relevant enough.\n",
      "--- ‚úçÔ∏è Executing node: generate_refined_response ---\n",
      "Bot:\n",
      "To reset your password, go to the login page and click 'Forgot Password'. Follow the instructions sent to your email.\n",
      "\n",
      "--- \n",
      "**Summary of Agent's Actions:**\n",
      "* **Your Question:** \"How do I reset my password?\"\n",
      "* **Interpreted as:** \"How do I reset my password?\"\n",
      "* **Retrieved Document:** \"How do I reset my password?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 4. Create and Load Data for RAG ---\n",
    "# Create a dummy CSV file for demonstration purposes.\n",
    "# csv_data = {\n",
    "#     'Question': [\n",
    "#         \"How do I reset my password?\",\n",
    "#         \"What are the system requirements for the software?\",\n",
    "#         \"How can I contact customer support?\",\n",
    "#         \"What is the return policy?\",\n",
    "#         \"Where can I find the user manual?\"\n",
    "#     ],\n",
    "#     'Answer': [\n",
    "#         \"To reset your password, go to the login page and click 'Forgot Password'. Follow the instructions sent to your email.\",\n",
    "#         \"The software requires Windows 10 or later, 8GB of RAM, and at least 5GB of free disk space.\",\n",
    "#         \"You can contact customer support by emailing help@example.com or by calling our hotline at 1-800-555-1234 during business hours.\",\n",
    "#         \"Our return policy allows for returns within 30 days of purchase, provided the product is in its original condition.\",\n",
    "#         \"The user manual is available for download in the 'Help' section of our website.\"\n",
    "#     ]\n",
    "# }\n",
    "# df = pd.DataFrame(csv_data)\n",
    "# df.to_csv('Helpdesk_bot_QA.csv', index=False)\n",
    "# print(\"Created 'Helpdesk_bot_QA.csv' with sample data.\")\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv('Helpdesk_bot_QA.csv')\n",
    "documents = [Document(page_content=f\"Question: {row['Question']}\\nAnswer: {row['Answer']}\") for index, row in df.iterrows()]\n",
    "\n",
    "# Create the FAISS vector store and a retriever that gets the single best result\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "# NOTE: We will call the vector store directly to get scores, not using as_retriever()\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "# --- 5. Define the Graph State ---\n",
    "# FIX: Updated state to pass the rephrased_question to the new response generator node.\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    question: str\n",
    "    rephrased_question: str\n",
    "    retrieved_document: Document\n",
    "\n",
    "# --- 6. Define the Graph Nodes ---\n",
    "def intent_and_retrieve(state: AgentState):\n",
    "    \"\"\"\n",
    "    Node to interpret user intent, retrieve a document, and pass all info to the state.\n",
    "    \"\"\"\n",
    "    print(\"--- ü§î Executing node: intent_and_retrieve ---\")\n",
    "    question = state['question']\n",
    "    \n",
    "    # Create the prompt for the native LLM call. note: still not sure how category gets created - mcd\n",
    "    template = \"\"\"Rephrase the user's question into a canonical question suitable for an FAQ document.\n",
    "You must respond with a JSON object. The most important key in this object should contain the rephrased question.\n",
    "\n",
    "Example:\n",
    "User question: I forgot my password, what do I do?\n",
    "JSON response: {{\"rephrased_question\": \"How do I reset my password?\", \"category\": \"account_management\"}}\n",
    "\n",
    "User question: {question}\n",
    "\"\"\"\n",
    "    prompt_text = template.format(question=question)\n",
    "\n",
    "    # Invoke the LLM using the native google-generativeai library to bypass LangChain errors\n",
    "    response = native_model.generate_content(prompt_text)\n",
    "    raw_response_text = response.text\n",
    "    print(f\"    Raw response from LLM: {raw_response_text}\")\n",
    "\n",
    "    # Parse the JSON response\n",
    "    if '```json' in raw_response_text:\n",
    "        json_str = raw_response_text.split('```json')[1].split('```')[0].strip()\n",
    "    else:\n",
    "        json_str = raw_response_text\n",
    "    response_data = json.loads(json_str)\n",
    "    \n",
    "    # Dynamically find the key containing the rephrased question\n",
    "    rephrased_question = None\n",
    "    for key in response_data.keys():\n",
    "        if \"question\" in key.lower():\n",
    "            rephrased_question = response_data[key]\n",
    "            break\n",
    "    if not rephrased_question:\n",
    "        rephrased_question = list(response_data.values())[0]\n",
    "\n",
    "    print(f\"    Rephrased question for retrieval: '{rephrased_question}'\")\n",
    "    \n",
    "    # FIX: Retrieve document with relevance score to handle irrelevant questions.\n",
    "    # We call the vector store directly to get similarity scores.\n",
    "    #note: phenomenon is that exact Q statements, even as rephrased Q still, will not get 100 similarity scores - mcd\n",
    "    docs_and_scores = vector_store.similarity_search_with_relevance_scores(rephrased_question, k=1)\n",
    "\n",
    "    retrieved_document = None\n",
    "    if docs_and_scores:\n",
    "        doc, score = docs_and_scores[0]\n",
    "        print(f\"    Top document retrieved with relevance score: {score:.4f}\")\n",
    "        # Set a threshold for relevance. Scores are 0-1, higher is better.\n",
    "        #lowered this down to .6 after testing against the highly related test cases. didn't go below this though; allows less false negatives while \n",
    "        #still not seeing any false positives\n",
    "        \n",
    "        if score > 0.6:  # Adjust this threshold as needed\n",
    "            retrieved_document = doc\n",
    "            print(\"    ‚úÖ Match is relevant enough.\")\n",
    "        else:\n",
    "            print(\"    ‚ùå Match is NOT relevant enough. Discarding.\")\n",
    "    else:\n",
    "        print(\"    No document found in vector store.\")\n",
    "    \n",
    "    return {\n",
    "        \"retrieved_document\": retrieved_document,\n",
    "        \"rephrased_question\": rephrased_question,\n",
    "        \"question\": question\n",
    "    }\n",
    "\n",
    "# FIX: Added a new node to generate the final, refined response.\n",
    "def generate_refined_response(state: AgentState):\n",
    "    \"\"\"\n",
    "    Node to generate a final, structured response for the user based on the retrieved document.\n",
    "    \"\"\"\n",
    "    print(\"--- ‚úçÔ∏è Executing node: generate_refined_response ---\")\n",
    "    question = state[\"question\"]\n",
    "    rephrased_question = state[\"rephrased_question\"]\n",
    "    doc = state['retrieved_document']\n",
    "    \n",
    "    # This 'if' statement now correctly handles cases where the retriever\n",
    "    # found a document but it was below the relevance threshold.\n",
    "    if not doc:\n",
    "        response_content = \"I'm sorry, I couldn't find a relevant answer in our documentation. Please try rephrasing your question.\"\n",
    "        return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "    # Parse the Question and Answer from the document's content\n",
    "    content = doc.page_content\n",
    "    retrieved_q = content.split('Answer:')[0].replace('Question:', '').strip()\n",
    "    retrieved_a = content.split('Answer:')[1].strip()\n",
    "\n",
    "    # Create the final structured response as requested\n",
    "    response_content = (\n",
    "        f\"{retrieved_a}\\n\\n\"\n",
    "        f\"--- \\n\"\n",
    "        f\"**Summary of Agent's Actions:**\\n\"\n",
    "        f\"* **Your Question:** \\\"{question}\\\"\\n\"\n",
    "        f\"* **Interpreted as:** \\\"{rephrased_question}\\\"\\n\"\n",
    "        f\"* **Retrieved Document:** \\\"{retrieved_q}\\\"\"\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# --- 7. Build the Graph ---\n",
    "# FIX: Updated the graph to use the new two-node structure.\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"intent_and_retriever\", intent_and_retrieve)\n",
    "workflow.add_node(\"generate_refined_response\", generate_refined_response)\n",
    "\n",
    "workflow.set_entry_point(\"intent_and_retriever\")\n",
    "workflow.add_edge(\"intent_and_retriever\", \"generate_refined_response\")\n",
    "workflow.add_edge(\"generate_refined_response\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "\n",
    "# --- 8. Run the Chatbot Interactively ---\n",
    "# FIX: Added timestamped log file creation and writing.\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"user_output_log_{timestamp}.txt\"\n",
    "print(f\"Logging chatbot responses to: {log_filename}\")\n",
    "\n",
    "print(\"\\nRefined RAG Chatbot is ready! Type your message or 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        initial_input = {\"question\": user_input, \"messages\": [HumanMessage(content=user_input)]}\n",
    "        \n",
    "        # The final state now contains the refined response\n",
    "        final_state = graph.invoke(initial_input)\n",
    "        \n",
    "        # Print the final message from the chatbot\n",
    "        final_response = final_state['messages'][-1].content\n",
    "        print(f\"Bot:\\n{final_response}\\n\")\n",
    "\n",
    "        # Append the response to the log file as a tuple string. want this to help either build in a history to an app and/or analyze trends and topics\n",
    "        try:\n",
    "            with open(log_filename, 'a', encoding='utf-8') as f:\n",
    "                # Create a tuple containing the single response string\n",
    "                response_tuple = (final_response,)\n",
    "                # Write the string representation of the tuple to the file\n",
    "                f.write(str(response_tuple) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"    [WARNING] Could not write to log file: {e}\")\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- üí• An error occurred during graph execution ---\")\n",
    "        print(f\"    ERROR DETAILS: {e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471ec21-7b49-4b37-9073-d9e65ab12521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
